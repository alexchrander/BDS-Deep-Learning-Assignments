# Green Patent Detection (PatentSBERTa)
### Active Learning + LLM → Human HITL

Binary classifier for detecting green/sustainable technology patent claims, built using PatentSBERTa with an active learning and Human-in-the-Loop (HITL) workflow.

---

## Project Structure

```
├── Assignment2_Green_Patent_Detection.ipynb  # Main notebook
├── llm_label.py                              # LLM inference script (Part C, HPC)
├── finetune.py                               # Fine-tuning script (Part D, HPC)
├── slurm_llm.sh                              # SLURM job for LLM labeling
├── slurm_finetune.sh                         # SLURM job for fine-tuning
├── pyproject.toml                            # Dependencies
├── csv/
│   ├── hitl_green_100.csv                    # 100 uncertain examples
│   ├── hitl_llm_labeled.csv                  # LLM suggestions
│   └── hitl_human_labeled.csv               # Final gold labels
├── embeddings/                               # Frozen PatentSBERTa embeddings (not in git)
│   ├── X_train.npy                           # Embeddings for train_silver
│   ├── X_pool.npy                            # Embeddings for pool_unlabeled
│   └── X_eval.npy                            # Embeddings for eval_silver
├── parquet/                                  # Dataset splits (not in git)
│   ├── patents_50k_green.parquet             # Balanced 50k sample
│   ├── train_silver.parquet                  # Training split (70%)
│   ├── pool_unlabeled.parquet                # Unlabeled pool (20%)
│   ├── eval_silver.parquet                   # Eval split (10%)
│   └── train_gold.parquet                    # Gold-enhanced training set
├── models/                                   # Fine-tuned model (not in git)
│   └── patentsberta-finetuned/
├── baseline_clf.pkl                          # Trained baseline classifier (not in git)
└── logs/                                     # SLURM job logs
```

> **Note:** The following are excluded from git via `.gitignore` due to file size. They are all regenerated by running the notebook and HPC scripts:
> - `embeddings/`
> - `parquet/`
> - `models/`
> - `baseline_clf.pkl`

---

## Setup

Install [uv](https://github.com/astral-sh/uv) and sync dependencies:

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
uv sync
source .venv/bin/activate
```

Create a `.env` file with your HuggingFace token:

```
HF_TOKEN=hf_xxxxxxxxxxxxxxxx
```

---

## How to Run

The project is split between the notebook (local) and HPC scripts. Follow the parts in order.

### Part A – Baseline Model (Notebook)
Run cells 1–6 in the notebook. Downloads the dataset, creates the 50k balanced sample, encodes with frozen PatentSBERTa, and trains a Logistic Regression baseline. Embeddings saved to `embeddings/`, classifier saved as `baseline_clf.pkl`.

### Part B – Uncertainty Sampling (Notebook)
Run cells 7–8. Computes uncertainty scores for `pool_unlabeled` and exports the 100 most uncertain examples to `csv/hitl_green_100.csv`.

### Part C – LLM → Human HITL

**Step 1 – LLM labeling on HPC:**
```bash
sbatch slurm_llm.sh
```
Runs `llm_label.py` using Mistral-7B-Instruct-v0.2 via vllm. Output saved to `csv/hitl_llm_labeled.csv`.

**Step 2 – Human review (Notebook):**
Copy `hitl_llm_labeled.csv` back locally and run cells 9a–9e. An interactive widget lets you review each claim and assign the final gold label. Output saved to `csv/hitl_human_labeled.csv`.

### Part D – Fine-tune PatentSBERTa

**Step 1 – Merge gold labels (Notebook):**
Run cell 10 to create `parquet/train_gold.parquet` (train_silver + gold_100).

**Step 2 – Fine-tuning on HPC:**
```bash
sbatch slurm_finetune.sh
```
Runs `finetune.py`. Fine-tunes PatentSBERTa for 1 epoch and saves model to `models/patentsberta-finetuned`.

---

## Results

### eval_silver
| | Precision | Recall | F1 | Accuracy |
|---|---|---|---|---|
| Part A – Baseline (frozen) | 0.77 | 0.77 | 0.77 | 0.77 |
| Part D – Fine-tuned | 0.81 | 0.81 | 0.81 | 0.81 |

Fine-tuning improved accuracy by 4 percentage points over the frozen baseline.

### gold_100
| | Precision | Recall | F1 | Accuracy |
|---|---|---|---|---|
| Fine-tuned | 0.57 | 0.67 | 0.52 | 0.62 |

Lower performance on gold_100 is expected — these were selected as the most uncertain examples by the baseline model.

---

## HITL Summary

- 100 uncertain examples labeled via Mistral-7B-Instruct-v0.2 → human review
- LLM labeled 95 as not green, 5 as green (72% low confidence)
- Human overrode the LLM in **6 out of 100 cases**, all from not green → green

---

## HuggingFace

- Model: [alexchrander/patent-sberta-green-finetuned](https://huggingface.co/alexchrander/patent-sberta-green-finetuned)
- Dataset: [alexchrander/patents-green-gold-dataset](https://huggingface.co/datasets/alexchrander/patents-green-gold-dataset)