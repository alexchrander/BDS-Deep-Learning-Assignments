{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3125bbf5",
   "metadata": {},
   "source": [
    "# Green Patent Detection: Advanced Architectures (Agents vs. QLoRA)\n",
    "\n",
    "## üîÑ Parts A & B ‚Äì Setup (Reused from Assignment 2)\n",
    "\n",
    "The baseline model, uncertainty scores, and 100 high-risk examples were already computed \n",
    "in Assignment 2. We reuse those artifacts directly here.\n",
    "\n",
    "**Dataset splits (from Assignment 2 Part A):**\n",
    "- `train_silver` ‚Äî 70% of the 50k sample (35,000 rows), used to train the baseline\n",
    "- `pool_unlabeled` ‚Äî 20% of the 50k sample (10,000 rows), used for uncertainty sampling\n",
    "- `eval_silver` ‚Äî 10% of the 50k sample (5,000 rows), used for evaluation\n",
    "\n",
    "**Reused artifacts:**\n",
    "- `baseline_clf.pkl` ‚Äî trained Logistic Regression on frozen PatentSBERTa embeddings\n",
    "- `embeddings/X_train.npy` ‚Äî frozen embeddings for train_silver\n",
    "- `embeddings/X_pool.npy` ‚Äî frozen embeddings for pool_unlabeled\n",
    "- `embeddings/X_eval.npy` ‚Äî frozen embeddings for eval_silver\n",
    "- `parquet/pool_unlabeled.parquet` ‚Äî the unlabeled pool\n",
    "- `parquet/train_silver.parquet` ‚Äî the training split\n",
    "- `parquet/eval_silver.parquet` ‚Äî the evaluation split\n",
    "- `csv/hitl_green_100.csv` ‚Äî the 100 most uncertain claims (u = 0.987‚Äì1.000)\n",
    "\n",
    "These feed directly into Part C where we replace the simple LLM labeling from \n",
    "Assignment 2 with a Multi-Agent System (MAS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c5cd229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pool size:        10000\n",
      "Train size:       35000\n",
      "Eval size:        5000\n",
      "High-risk claims: 100\n",
      "Uncertainty range: 0.987 ‚Äì 1.000\n"
     ]
    }
   ],
   "source": [
    "# Load dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# Load baseline model (logistic regression)\n",
    "clf         = pickle.load(open(\"baseline_clf.pkl\", \"rb\"))\n",
    "\n",
    "# Load frozen embeddings\n",
    "X_train     = np.load(\"embeddings/X_train.npy\")\n",
    "X_pool      = np.load(\"embeddings/X_pool.npy\")\n",
    "X_eval      = np.load(\"embeddings/X_eval.npy\")\n",
    "\n",
    "# Load splits from the parquet file\n",
    "train_silver = pd.read_parquet(\"parquet/train_silver.parquet\")\n",
    "pool        = pd.read_parquet(\"parquet/pool_unlabeled.parquet\")\n",
    "eval_silver  = pd.read_parquet(\"parquet/eval_silver.parquet\")\n",
    "\n",
    "# Load the 100 high-risk claims\n",
    "hitl_100    = pd.read_csv(\"csv/hitl_green_100.csv\")\n",
    "\n",
    "print(f\"Pool size:        {len(pool)}\")\n",
    "print(f\"Train size:       {len(train_silver)}\")\n",
    "print(f\"Eval size:        {len(eval_silver)}\")\n",
    "print(f\"High-risk claims: {len(hitl_100)}\")\n",
    "print(f\"Uncertainty range: {hitl_100['u'].min():.3f} ‚Äì {hitl_100['u'].max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b140c052",
   "metadata": {},
   "source": [
    "## üîÄ Part C: Choose Your Advanced Path\n",
    "\n",
    "The Multi-Agent System (MAS) was implemented as a three-agent debate pipeline using \n",
    "vllm for inference on HPC, with LangGraph orchestrating the state flow between agents.\n",
    "\n",
    "**Files:**\n",
    "- `mas_label.py` ‚Äî the MAS inference script\n",
    "- `slurm_mas.sh` ‚Äî the SLURM job file\n",
    "\n",
    "**Agents:**\n",
    "- **Advocate** (Mistral-7B-Instruct-v0.2) ‚Äî argues FOR green classification, \n",
    "looking for environmental benefits and sustainability aspects\n",
    "- **Skeptic** (Qwen2.5-7B-Instruct) ‚Äî argues AGAINST green classification, \n",
    "looking for greenwashing or weak green signals\n",
    "- **Judge** (Meta-Llama-3-8B-Instruct) ‚Äî weighs both arguments and produces \n",
    "the final JSON label, confidence score, and rationale\n",
    "\n",
    "**Results:**\n",
    "- 98 out of 100 claims parsed successfully (2 failed)\n",
    "- Label distribution: 51 not green, 47 green\n",
    "- Confidence: 76 medium, 18 high, 4 low\n",
    "\n",
    "**Comparison with Assignment 2 (simple Mistral):**\n",
    "\n",
    "| | Not Green | Green | Low Confidence |\n",
    "|---|---|---|---|\n",
    "| Assignment 2 (Mistral) | 95 | 5 | 72% |\n",
    "| Assignment 3 (MAS) | 51 | 47 | 4% |\n",
    "\n",
    "The debate structure produced significantly more balanced and confident labels compared \n",
    "to the single LLM approach ‚Äî the Advocate agent pushed back against the Skeptic's \n",
    "tendency to default to not green, resulting in a more nuanced labeling process.\n",
    "\n",
    "**Output file:** `csv/mas_labeled.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71a4b70",
   "metadata": {},
   "source": [
    "## üèÅ Part D: Human Review & Final Integration\n",
    "\n",
    "Before starting the human review, we first clean up the MAS output by dropping the \n",
    "Assignment 2 LLM columns that are no longer needed, then proceed with the same HITL \n",
    "widget as Assignment 2.\n",
    "\n",
    "### Human Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1cbd1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns before cleanup: ['doc_id', 'text', 'p_green', 'u', 'llm_green_suggested', 'llm_confidence', 'llm_rationale', 'is_green_human', 'notes', 'mas_green_suggested', 'mas_confidence', 'mas_rationale', 'advocate_arg', 'skeptic_arg']\n",
      "Shape: (100, 14)\n",
      "Failed parses before cleanup: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after cleanup: ['doc_id', 'text', 'p_green', 'u', 'is_green_human', 'notes', 'mas_green_suggested', 'mas_confidence', 'mas_rationale', 'advocate_arg', 'skeptic_arg']\n",
      "Shape: (100, 11)\n",
      "Failed parses after cleanup: 0\n"
     ]
    }
   ],
   "source": [
    "df_mas = pd.read_csv(\"csv/mas_labeled.csv\")\n",
    "\n",
    "print(f\"Columns before cleanup: {df_mas.columns.tolist()}\")\n",
    "print(f\"Shape: {df_mas.shape}\")\n",
    "print(f\"Failed parses before cleanup: {df_mas['mas_green_suggested'].isna().sum()}\")\n",
    "\n",
    "# Fill the 2 failed parses with sensible defaults\n",
    "df_mas[\"mas_green_suggested\"] = df_mas[\"mas_green_suggested\"].fillna(1)\n",
    "df_mas[\"mas_confidence\"]      = df_mas[\"mas_confidence\"].fillna(\"low\")\n",
    "df_mas[\"mas_rationale\"]       = df_mas[\"mas_rationale\"].fillna(\"Failed to parse ‚Äî defaulted to green.\")\n",
    "\n",
    "# Drop Assignment 2 LLM columns ‚Äî not needed for Assignment 3\n",
    "df_mas = df_mas.drop(columns=[\"llm_green_suggested\", \"llm_confidence\", \"llm_rationale\"])\n",
    "\n",
    "# Save cleaned version\n",
    "df_mas.to_csv(\"csv/mas_labeled.csv\", index=False)\n",
    "\n",
    "print(f\"Columns after cleanup: {df_mas.columns.tolist()}\")\n",
    "print(f\"Shape: {df_mas.shape}\")\n",
    "print(f\"Failed parses after cleanup: {df_mas['mas_green_suggested'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d73a22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Load the cleaned MAS labeled file\n",
    "df_review = pd.read_csv(\"csv/mas_labeled.csv\")\n",
    "\n",
    "# Tracks which row we're currently reviewing\n",
    "state = {\"idx\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be4857e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display elements\n",
    "progress     = widgets.Label()\n",
    "claim_text   = widgets.Textarea(layout=widgets.Layout(width=\"100%\", height=\"150px\"), disabled=True)\n",
    "advocate_box = widgets.Textarea(layout=widgets.Layout(width=\"100%\", height=\"80px\"), disabled=True)\n",
    "skeptic_box  = widgets.Textarea(layout=widgets.Layout(width=\"100%\", height=\"80px\"), disabled=True)\n",
    "mas_label    = widgets.Label()\n",
    "mas_conf     = widgets.Label()\n",
    "mas_rat      = widgets.Textarea(layout=widgets.Layout(width=\"100%\", height=\"80px\"), disabled=True)\n",
    "notes_box    = widgets.Textarea(placeholder=\"Optional: add a note (especially if you disagree)\",\n",
    "                                layout=widgets.Layout(width=\"100%\", height=\"60px\"))\n",
    "\n",
    "# Buttons\n",
    "btn_green    = widgets.Button(description=\"1 - Green\",     button_style=\"success\")\n",
    "btn_notgreen = widgets.Button(description=\"0 - Not Green\", button_style=\"danger\")\n",
    "btn_prev     = widgets.Button(description=\"‚Üê Previous\")\n",
    "out          = widgets.Output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bb10591",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"csv/mas_human_labeled.csv\"\n",
    "\n",
    "def show_row(idx):\n",
    "    row = df_review.iloc[idx]\n",
    "    progress.value    = f\"Claim {idx + 1} / {len(df_review)}\"\n",
    "    claim_text.value  = str(row[\"text\"])\n",
    "    advocate_box.value = str(row[\"advocate_arg\"])\n",
    "    skeptic_box.value  = str(row[\"skeptic_arg\"])\n",
    "    mas_label.value   = f\"MAS suggested: {int(row['mas_green_suggested']) if pd.notna(row['mas_green_suggested']) else 'N/A'}\"\n",
    "    mas_conf.value    = f\"MAS confidence: {row['mas_confidence']}\"\n",
    "    mas_rat.value     = str(row[\"mas_rationale\"])\n",
    "    notes_box.value   = str(row[\"notes\"]) if pd.notna(row[\"notes\"]) else \"\"\n",
    "\n",
    "def save_and_advance(label):\n",
    "    idx = state[\"idx\"]\n",
    "    df_review.at[idx, \"is_green_human\"] = label\n",
    "    df_review.at[idx, \"notes\"]          = notes_box.value\n",
    "    df_review.to_csv(OUTPUT_PATH, index=False)\n",
    "    with out:\n",
    "        clear_output()\n",
    "        print(f\"Saved: claim {idx + 1} ‚Üí {label}\")\n",
    "    state[\"idx\"] = min(idx + 1, len(df_review) - 1)\n",
    "    show_row(state[\"idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea2c8116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa5bf2b1ec5415b9d1142d09e8f6246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Claim 1 / 100'), Label(value='Claim text:'), Textarea(value='1. A processor compri‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Connect buttons to logic\n",
    "btn_green.on_click(lambda _: save_and_advance(1))\n",
    "btn_notgreen.on_click(lambda _: save_and_advance(0))\n",
    "btn_prev.on_click(lambda _: [state.update({\"idx\": max(state[\"idx\"] - 1, 0)}), show_row(state[\"idx\"])])\n",
    "\n",
    "# Start at first unlabeled row so you can safely resume after interruptions\n",
    "first_unlabeled = df_review[\"is_green_human\"].isna().idxmax()\n",
    "state[\"idx\"] = first_unlabeled if pd.isna(df_review.at[first_unlabeled, \"is_green_human\"]) else 0\n",
    "show_row(state[\"idx\"])\n",
    "\n",
    "# Render the widget\n",
    "display(widgets.VBox([\n",
    "    progress,\n",
    "    widgets.Label(\"Claim text:\"),    claim_text,\n",
    "    widgets.Label(\"Advocate:\"),      advocate_box,\n",
    "    widgets.Label(\"Skeptic:\"),       skeptic_box,\n",
    "    widgets.Label(\"MAS output:\"),    mas_label, mas_conf,\n",
    "    widgets.Label(\"MAS rationale:\"), mas_rat,\n",
    "    widgets.Label(\"Your notes:\"),    notes_box,\n",
    "    widgets.HBox([btn_notgreen, btn_green, btn_prev]),\n",
    "    out\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2475916b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total overrides: 9 / 100\n",
      "\n",
      "--- Example 1 ---\n",
      "Claim:          1. A radio communications system comprising: a first radio base station to convert multimedia broadcast multicast service (MBMS) data to unicast data and transmit the unicast data to an intermediate s...\n",
      "MAS suggested:  0.0 (medium confidence)\n",
      "MAS rationale:  While the system may improve data transmission efficiency, the lack of explicit environmental metrics, energy efficiency analysis, and consideration of broader environmental impacts makes it difficult to classify this patent as genuinely green or sustainable technology.\n",
      "Human label:    1.0\n",
      "Notes:          The advocate seems pretty convincing with the argument of energy savings.\n",
      "\n",
      "--- Example 2 ---\n",
      "Claim:          1. A method comprising: operating an aerial vehicle in a hover-flight orientation, wherein the aerial vehicle is connected to a tether that defines a tether sphere having a radius based on a length of...\n",
      "MAS suggested:  nan (nan confidence)\n",
      "MAS rationale:  You are an impartial judge evaluating whether a patent claim describes green/sustainable technology.\n",
      "You have received arguments from two experts. Weigh both arguments and make a final decision.\n",
      "Patent claim:\n",
      "1. A method comprising: operating an aerial vehicle in a hover-flight orientation, wherein the aerial vehicle is connected to a tether that defines a tether sphere having a radius based on a length of the tether, wherein the tether is connected to a ground station; while the aerial vehicle is in the hover-flight orientation, positioning the aerial vehicle at a first location that is substantially on the tether sphere, wherein the first location is substantially downwind of the ground station; transitioning the aerial vehicle from the hover-flight orientation to a forward-flight orientation, such that the aerial vehicle moves from the tether sphere, wherein the aerial vehicle has attached flow, and wherein a tension of the tether is reduced; and operating the aerial vehicle in the forward-flight orientation to ascend at an angle of ascent to a second location that is substantially on the tether sphere, wherein the second location is substantially downwind of the ground station.\n",
      "Advocate argues FOR green classification:\n",
      "This patent claim describes a method of operating an aerial vehicle using a tether system, which allows for hovering and controlled forward flight while maintaining a connection to a ground station. The use of a tether system reduces the need for continuous fuel consumption for flight, making it a more energy-efficient and sustainable solution compared to traditional aircraft. Additionally, the method's focus on hovering and forward flight in a downwind direction can potentially harness wind energy to aid in the aerial vehicle's ascension, further enhancing its green and sustainable characteristics.\n",
      "Skeptic argues AGAINST green classification:\n",
      "This patent claim focuses on the operational mechanics of an aerial vehicle and its movement relative to a tether and ground station, but it does not provide specific details on the environmental benefits or sustainability of the technology. Without evidence that the aerial vehicle or its operation significantly reduces environmental impact, such as through energy efficiency, emissions reduction, or the use of renewable energy sources, the claim lacks substantial green signals and cannot be classified as green/sustainable technology. The claim appears more focused on the vehicle's operational dynamics rather than its environmental footprint.\n",
      "Respond in this exact JSON format:\n",
      "{\n",
      "    \"mas_green_suggested\": 0 or 1,\n",
      "    \"mas_confidence\": \"low\", \"medium\", or \"high\",\n",
      "    \"mas_rationale\": \"1-3 sentences explaining your final decision\"\n",
      "} {\n",
      "    \"mas_green_suggested\": 0,\n",
      "    \"mas_confidence\": \"medium\",\n",
      "Human label:    1.0\n",
      "Notes:          Advocate brings a good point as this product is more energy-efficient and sustainable.\n",
      "\n",
      "--- Example 3 ---\n",
      "Claim:          1. A reusable, multi-purpose bag, comprising: a flexible, resiliently deformable body comprising: wherein at least a portion of the anterior panel is bonded to at least a portion of the posterior pane...\n",
      "MAS suggested:  0.0 (medium confidence)\n",
      "MAS rationale:  While the reusable bag has some potential environmental benefits, the lack of clear evidence on its lifecycle assessment and environmental impact, particularly regarding the production and disposal of the bonding material, prevents me from classifying it as green/sustainable technology. The potential environmental impacts of the manufacturing process and end-of-life management of the bag are significant concerns that need to be addressed before it can be considered a sustainable alternative.\n",
      "Human label:    1.0\n",
      "Notes:          The advocate brings a good argument about reducing plastic waste and polution.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_human = pd.read_csv(\"csv/mas_human_labeled.csv\")\n",
    "\n",
    "# Find rows where human label differs from MAS suggestion\n",
    "overrides = df_human[df_human[\"is_green_human\"] != df_human[\"mas_green_suggested\"]]\n",
    "\n",
    "print(f\"Total overrides: {len(overrides)} / {len(df_human)}\")\n",
    "print()\n",
    "\n",
    "# Print 3 examples for the README\n",
    "for i, (_, row) in enumerate(overrides.head(3).iterrows()):\n",
    "    print(f\"--- Example {i+1} ---\")\n",
    "    print(f\"Claim:          {row['text'][:200]}...\")\n",
    "    print(f\"MAS suggested:  {row['mas_green_suggested']} ({row['mas_confidence']} confidence)\")\n",
    "    print(f\"MAS rationale:  {row['mas_rationale']}\")\n",
    "    print(f\"Human label:    {row['is_green_human']}\")\n",
    "    print(f\"Notes:          {row['notes']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce570194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assignment 2 (simple Mistral) agreement with human: 94.0%\n",
      "Assignment 3 (MAS) agreement with human:            92.0%\n"
     ]
    }
   ],
   "source": [
    "# Load the human-labeled files for both assignments\n",
    "df_a2 = pd.read_csv(\"../Assignment2/csv/hitl_human_labeled.csv\")\n",
    "df_a3 = pd.read_csv(\"csv/mas_human_labeled.csv\")\n",
    "\n",
    "# Fix the 2 failed parses in the MAS file\n",
    "df_a3[\"mas_green_suggested\"] = df_a3[\"mas_green_suggested\"].fillna(1)\n",
    "\n",
    "# Agreement = rows where AI suggestion matches human label\n",
    "a2_agreement = (df_a2[\"llm_green_suggested\"] == df_a2[\"is_green_human\"]).mean() * 100\n",
    "a3_agreement = (df_a3[\"mas_green_suggested\"] == df_a3[\"is_green_human\"]).mean() * 100\n",
    "\n",
    "print(f\"Assignment 2 (simple Mistral) agreement with human: {a2_agreement:.1f}%\")\n",
    "print(f\"Assignment 3 (MAS) agreement with human:            {a3_agreement:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96a5ad8",
   "metadata": {},
   "source": [
    "### Final Intergration / Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98e721e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_silver rows: 35000\n",
      "gold_100 rows:     100\n",
      "combined rows:     35100\n",
      "is_green_gold\n",
      "1.0    17553\n",
      "0.0    17547\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the MAS human reviewed labels and train_silver\n",
    "df_gold  = pd.read_csv(\"csv/mas_human_labeled.csv\")\n",
    "df_train = pd.read_parquet(\"parquet/train_silver.parquet\")\n",
    "\n",
    "# Rename human label to is_green_gold for consistency\n",
    "df_gold[\"is_green_gold\"] = df_gold[\"is_green_human\"]\n",
    "\n",
    "# Give train_silver the same column name\n",
    "df_train[\"is_green_gold\"] = df_train[\"is_green_silver\"]\n",
    "\n",
    "# Concatenate train_silver + gold_100 into one training set\n",
    "# The 100 examples came from the pool split which is different from the training split\n",
    "# This means that the 100 examples gets added to the training set because it has no doc id's to overwrite\n",
    "df_combined = pd.concat(\n",
    "    [df_train, df_gold[[\"doc_id\", \"text\", \"is_green_gold\"]]],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "print(f\"train_silver rows: {len(df_train)}\")\n",
    "print(f\"gold_100 rows:     {len(df_gold)}\")\n",
    "print(f\"combined rows:     {len(df_combined)}\")\n",
    "print(df_combined[\"is_green_gold\"].value_counts())\n",
    "\n",
    "df_combined.to_parquet(\"parquet/train_gold.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78e22bc",
   "metadata": {},
   "source": [
    "Now that `train_gold.parquet` has been created combining `train_silver` and the 100 MAS \n",
    "gold labels from the three-agent debate pipeline, the fine-tuning step needs to be run \n",
    "on HPC. The same `finetune.py` and `slurm_finetune.sh` from Assignment 2 are reused here \n",
    "since the fine-tuning process is identical ‚Äî the only difference is the gold labels now \n",
    "come from the MAS instead of the simple Mistral prompt.\n",
    "\n",
    "Follow these steps before continuing:\n",
    "\n",
    "1. Make sure `train_gold.parquet` and `mas_human_labeled.csv` are available in your HPC project folder\n",
    "2. Submit the SLURM job: `sbatch slurm_finetune.sh`\n",
    "3. Once complete, copy the saved model folder back to your local project: `models/patentsberta-finetuned`\n",
    "\n",
    "Then continue with Part E below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c448058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- eval_silver ---\n",
      "              precision    recall  f1-score   support\n",
      "   not green       0.81      0.80      0.80      2500\n",
      "       green       0.80      0.81      0.81      2500\n",
      "    accuracy                           0.81      5000\n",
      "   macro avg       0.81      0.81      0.81      5000\n",
      "weighted avg       0.81      0.81      0.81      5000\n",
      "\n",
      "--- gold_100 ---\n",
      "              precision    recall  f1-score   support\n",
      "   not green       0.49      0.51      0.50        47\n",
      "       green       0.55      0.53      0.54        53\n",
      "    accuracy                           0.52       100\n",
      "   macro avg       0.52      0.52      0.52       100\n",
      "weighted avg       0.52      0.52      0.52       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Results from the finetuning process\n",
    "\n",
    "# Fine-tuning results from HPC (finetune.py output)\n",
    "print(\"\"\"\n",
    "--- eval_silver ---\n",
    "              precision    recall  f1-score   support\n",
    "   not green       0.81      0.80      0.80      2500\n",
    "       green       0.80      0.81      0.81      2500\n",
    "    accuracy                           0.81      5000\n",
    "   macro avg       0.81      0.81      0.81      5000\n",
    "weighted avg       0.81      0.81      0.81      5000\n",
    "\n",
    "--- gold_100 ---\n",
    "              precision    recall  f1-score   support\n",
    "   not green       0.49      0.51      0.50        47\n",
    "       green       0.55      0.53      0.54        53\n",
    "    accuracy                           0.52       100\n",
    "   macro avg       0.52      0.52      0.52       100\n",
    "weighted avg       0.52      0.52      0.52       100\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521dbf69",
   "metadata": {},
   "source": [
    "## üìä Part E: Comparative Analysis (Required)\n",
    "\n",
    "### Model Comparison\n",
    "\n",
    "| Model Version | Training Data Source | Precision | Recall | F1 | Accuracy |\n",
    "|---|---|---|---|---|---|\n",
    "| 1. Baseline | Frozen Embeddings (No Fine-tuning) | 0.77 | 0.77 | 0.77 | 0.77 |\n",
    "| 2. Assignment 2 Model | Fine-tuned on Silver + Gold (Simple LLM) | 0.81 | 0.81 | 0.81 | 0.81 |\n",
    "| 3. Assignment 3 Model | Fine-tuned on Silver + Gold (MAS) | 0.81 | 0.81 | 0.81 | 0.81 |\n",
    "\n",
    "### gold_100 Performance\n",
    "\n",
    "| Model Version | Precision | Recall | F1 | Accuracy |\n",
    "|---|---|---|---|---|\n",
    "| Assignment 2 Model | 0.57 | 0.67 | 0.52 | 0.62 |\n",
    "| Assignment 3 Model | 0.52 | 0.52 | 0.52 | 0.52 |\n",
    "\n",
    "### Reflection\n",
    "\n",
    "Both the Assignment 2 and Assignment 3 models achieved identical performance on \n",
    "eval_silver (0.81 F1), suggesting that the quality of the gold labeling method has \n",
    "minimal impact on overall model performance when the gold set represents only 0.28% \n",
    "of the total training data. However, the MAS produced notably more balanced and \n",
    "confident labels (47 green vs 5 green in Assignment 2), indicating that the debate \n",
    "structure leads to more nuanced annotation. The effort of implementing a multi-agent \n",
    "system did not translate into a better downstream model on eval_silver, but may be \n",
    "more impactful in scenarios where the gold labeled set is larger or the silver labels \n",
    "are noisier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dae06c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8816f23340f642bda620f39788b24f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c7e50af773448291962edb96d45538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model uploaded successfully\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load HuggingFace token from .env file\n",
    "load_dotenv()\n",
    "TOKEN    = os.getenv(\"HF_TOKEN\")\n",
    "USERNAME = \"alexchrander\"\n",
    "\n",
    "# Set to True only when you want to upload ‚Äî prevents accidental re-uploads\n",
    "UPLOAD_TO_HF = False\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "if UPLOAD_TO_HF:\n",
    "    api.create_repo(\n",
    "        repo_id=f\"{USERNAME}/patent-sberta-green-finetuned-mas\",\n",
    "        token=TOKEN,\n",
    "        exist_ok=True\n",
    "    )\n",
    "    api.upload_folder(\n",
    "        folder_path=\"models/patentsberta-finetuned\",\n",
    "        repo_id=f\"{USERNAME}/patent-sberta-green-finetuned-mas\",\n",
    "        token=TOKEN\n",
    "    )\n",
    "    print(\"Model uploaded successfully\")\n",
    "else:\n",
    "    print(\"Skipping upload ‚Äî set UPLOAD_TO_HF = True to upload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a869cc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset uploaded successfully\n"
     ]
    }
   ],
   "source": [
    "if UPLOAD_TO_HF:\n",
    "    api.create_repo(\n",
    "        repo_id=f\"{USERNAME}/patents-green-mas-dataset\",\n",
    "        repo_type=\"dataset\",\n",
    "        token=TOKEN,\n",
    "        exist_ok=True\n",
    "    )\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=\"csv/mas_human_labeled.csv\",\n",
    "        path_in_repo=\"mas_human_labeled.csv\",\n",
    "        repo_id=f\"{USERNAME}/patents-green-mas-dataset\",\n",
    "        repo_type=\"dataset\",\n",
    "        token=TOKEN\n",
    "    )\n",
    "    print(\"Dataset uploaded successfully\")\n",
    "else:\n",
    "    print(\"Skipping upload ‚Äî set UPLOAD_TO_HF = True to upload\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
